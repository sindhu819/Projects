{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group1-Milestone2-LSA-Models_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sindhu819/Projects/blob/master/Group1_Milestone2_LSA_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBxok8yfoAqo",
        "colab_type": "text"
      },
      "source": [
        "## <font color =blue> Milestone - 2 \n",
        "\n",
        "Select one sub-class from each of your two assigned super-classes. Use these two selected sub-classes as your testing data. Use all the data minus the two selected test sub-classes as your training data. Repeat for all 5 x 5 =25 pairs of subclass images. Your algorithm should be initialized before it is being trained on a new training dataset in each of these 25 trials. All results should be based on the SAME algorithm you designed.\n",
        "\n",
        "\n",
        "<font color=blue>Group 1\n",
        "Super Class : Household Electronics, Household Furniture\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDTtXygUk-Ry",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Installing the required packages "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpzU22M8Stx2",
        "colab_type": "code",
        "outputId": "a2115660-e42b-4c6f-f530-514c31115069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.24)] [Waiting for headers] [1 I\r0% [Connecting to archive.ubuntu.com (91.189.88.24)] [Waiting for headers] [Wai\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.24)]\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.24)]\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [75.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [794 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [760 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,742 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [10.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,056 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [32.7 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,322 kB]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [840 kB]\n",
            "Fetched 6,904 kB in 4s (1,556 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZz3CyTplUrW",
        "colab_type": "text"
      },
      "source": [
        "#<font color=blue> Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J41uN1nDSyuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(os.environ[\"SPARK_HOME\"])# SPARK_HOME\n",
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.linalg import Vectors, DenseVector\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "Memory_limit = \"12g\"\n",
        "spark = SparkSession.builder.appName(\"Foo\").config(\"spark.executor.memory\", Memory_limit).config(\"spark.driver.memory\", Memory_limit).getOrCreate()\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n16niawUldMg",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Creating the Pydrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQrodgUUS63S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFwHZzDZmd88",
        "colab_type": "text"
      },
      "source": [
        "## <font color =blue> Mounting the Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wu8Td9ATfe4",
        "colab_type": "code",
        "outputId": "b20d0921-478b-4559-fa8d-7de98b3bd773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANdTIT3xmqYY",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Reading the Parquet file:\n",
        "<font color=blue>For the Milestone1, we saved the pre-processed data in parquet format. In miletstone 2, we directly used that parquet file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoyQ1-MZTfle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=spark.read.parquet(\"/content/drive/My Drive/Project/LSA/Dataset/Cifar.parquet/*\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5AfgUHiUHI4",
        "colab_type": "code",
        "outputId": "54a62cf8-d536-451f-dbae-1808591a06f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z02DGLgwUJNl",
        "colab_type": "code",
        "outputId": "c5693a2a-6d37-4334-a3c8-8d1361e27ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "data.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+-----------+----------+\n",
            "|coarse_labels|                data|fine_labels|labelindex|\n",
            "+-------------+--------------------+-----------+----------+\n",
            "|            5|[1.0,0.9960784313...|         39|       0.0|\n",
            "|            6|[0.79215686274509...|         94|       1.0|\n",
            "|            5|[1.0,1.0,0.964705...|         22|       0.0|\n",
            "|            6|[0.26666666666666...|         84|       1.0|\n",
            "|            6|[0.98039215686274...|         94|       1.0|\n",
            "|            5|[1.0,1.0,1.0,1.0,...|         40|       0.0|\n",
            "|            5|[0.28627450980392...|         39|       0.0|\n",
            "|            5|[0.0,0.0,0.0,0.0,...|         86|       0.0|\n",
            "|            6|[0.57647058823529...|          5|       1.0|\n",
            "|            6|[0.92549019607843...|         25|       1.0|\n",
            "|            6|[0.98823529411764...|         25|       1.0|\n",
            "|            5|[0.23921568627450...|         22|       0.0|\n",
            "|            5|[0.02352941176470...|         39|       0.0|\n",
            "|            6|[0.87843137254901...|          5|       1.0|\n",
            "|            5|[0.90980392156862...|         40|       0.0|\n",
            "|            5|[1.0,1.0,1.0,1.0,...|         22|       0.0|\n",
            "|            5|[0.92156862745098...|         39|       0.0|\n",
            "|            5|[0.82745098039215...|         40|       0.0|\n",
            "|            6|[0.38431372549019...|          5|       1.0|\n",
            "|            6|[0.68235294117647...|         94|       1.0|\n",
            "+-------------+--------------------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPfS5hEDvGCP",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Random Forest\n",
        "\n",
        "<font color=blue>\n",
        "We modelled the data using Random forest. In this model the training set contain 4 fine labels from each super class, where as test dataset contains 1 finelabels from each super class. It is being implemented using for loop. Here for predicting and training time  used time.clock to get the CPU processing time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ACbBfeQThUQ",
        "colab_type": "code",
        "outputId": "66ac2362-89a6-4ceb-e7d3-55bf9d9512bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "for i in [22,39,40,86,87]:\n",
        "  for j in [5,20,25,84,94]:\n",
        "    train_df = data.filter(~col('fine_labels').isin([i,j]))\n",
        "    test_df  = data.filter(col('fine_labels').isin([i,j]))\n",
        "    rfc=RandomForestClassifier(featuresCol=\"data\", labelCol=\"labelindex\",maxDepth=5,numTrees=200)\n",
        "    start=time.clock()\n",
        "    rfc_model=rfc.fit(train_df)\n",
        "    end=time.clock()\n",
        "    print(\"train_time:\",end-start)\n",
        "    start1=time.clock()\n",
        "    rfc_preds=rfc_model.transform(test_df)\n",
        "    end1=time.clock()\n",
        "    print(\"predict_time:\",end1-start1)\n",
        "    rfc_eval=BinaryClassificationEvaluator(labelCol=\"labelindex\")\n",
        "\n",
        "    print (\"Random Forest: {}%,{},{}\".format(rfc_eval.evaluate(rfc_preds)*100,i,j))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_time: 0.013842999999999606\n",
            "predict_time: 0.005681000000000047\n",
            "Random Forest: 63.79749999999995%,22,5\n",
            "train_time: 0.019735999999999976\n",
            "predict_time: 0.006674000000000291\n",
            "Random Forest: 43.77236111111109%,22,20\n",
            "train_time: 0.02089199999999991\n",
            "predict_time: 0.004169000000000089\n",
            "Random Forest: 57.20583333333329%,22,25\n",
            "train_time: 0.016125999999999863\n",
            "predict_time: 0.00603100000000012\n",
            "Random Forest: 64.95916666666666%,22,84\n",
            "train_time: 0.017132000000000147\n",
            "predict_time: 0.0041679999999999495\n",
            "Random Forest: 71.12694444444448%,22,94\n",
            "train_time: 0.014983000000000857\n",
            "predict_time: 0.005679999999999907\n",
            "Random Forest: 66.73791666666662%,39,5\n",
            "train_time: 0.020322000000000173\n",
            "predict_time: 0.007651999999999326\n",
            "Random Forest: 45.63527777777775%,39,20\n",
            "train_time: 0.02213699999999985\n",
            "predict_time: 0.007384999999999309\n",
            "Random Forest: 59.29263888888894%,39,25\n",
            "train_time: 0.014661000000000257\n",
            "predict_time: 0.009856000000000087\n",
            "Random Forest: 62.50222222222222%,39,84\n",
            "train_time: 0.02133099999999999\n",
            "predict_time: 0.008370999999999462\n",
            "Random Forest: 59.80430555555554%,39,94\n",
            "train_time: 0.01829900000000073\n",
            "predict_time: 0.011435000000000528\n",
            "Random Forest: 61.555555555555586%,40,5\n",
            "train_time: 0.0196430000000003\n",
            "predict_time: 0.008168999999999649\n",
            "Random Forest: 46.669444444444444%,40,20\n",
            "train_time: 0.017513000000000112\n",
            "predict_time: 0.004248000000000474\n",
            "Random Forest: 57.74722222222224%,40,25\n",
            "train_time: 0.014508000000000187\n",
            "predict_time: 0.00696600000000025\n",
            "Random Forest: 62.7433333333334%,40,84\n",
            "train_time: 0.021702000000000332\n",
            "predict_time: 0.00712500000000027\n",
            "Random Forest: 68.77708333333335%,40,94\n",
            "train_time: 0.016657999999999618\n",
            "predict_time: 0.01158199999999976\n",
            "Random Forest: 62.45027777777783%,86,5\n",
            "train_time: 0.020224999999999937\n",
            "predict_time: 0.006609000000000087\n",
            "Random Forest: 70.12527777777784%,86,20\n",
            "train_time: 0.02571300000000054\n",
            "predict_time: 0.004607999999999279\n",
            "Random Forest: 63.93444444444447%,86,25\n",
            "train_time: 0.01854900000000015\n",
            "predict_time: 0.006527999999999423\n",
            "Random Forest: 63.232361111111125%,86,84\n",
            "train_time: 0.016485999999999557\n",
            "predict_time: 0.009673000000000265\n",
            "Random Forest: 65.42083333333338%,86,94\n",
            "train_time: 0.015158000000000449\n",
            "predict_time: 0.004715999999999276\n",
            "Random Forest: 65.57416666666673%,87,5\n",
            "train_time: 0.019612000000000407\n",
            "predict_time: 0.005758000000000152\n",
            "Random Forest: 41.70777777777777%,87,20\n",
            "train_time: 0.020148999999999972\n",
            "predict_time: 0.0065189999999999415\n",
            "Random Forest: 70.18777777777775%,87,25\n",
            "train_time: 0.018641999999999825\n",
            "predict_time: 0.0068190000000001305\n",
            "Random Forest: 70.99527777777774%,87,84\n",
            "train_time: 0.013400999999999996\n",
            "predict_time: 0.006159000000000248\n",
            "Random Forest: 74.2644444444444%,87,94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w089PgoWpy7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "train=data.filter(~col('fine_labels').isin([87,94]))\n",
        "test= data.filter(col('fine_labels').isin([87,94]))\n",
        "rfc1=RandomForestClassifier(featuresCol=\"data\", labelCol=\"labelindex\",numTrees=200)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBoVTSnvqW6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoVh2j_O_Ylq",
        "colab_type": "text"
      },
      "source": [
        "#<font color= blue > Train time  evaluation fot the best subset using Random Forest\n",
        "\n",
        "<font color=blue>we are comparing the cpu time for all algorithms for both train and prediction methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYEKle72Le8O",
        "colab_type": "text"
      },
      "source": [
        "#<font color=blue> train time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApaA8K5hja7L",
        "colab_type": "code",
        "outputId": "8992398d-1355-4d2a-bc9d-852bfc65490a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start=time.clock()\n",
        "rfc_model1=rfc1.fit(train)\n",
        "end=time.clock()\n",
        "print(\"train:\" ,end-start)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: 0.03934300000000013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4r6L9drLlrN",
        "colab_type": "text"
      },
      "source": [
        "## <font color=blue> Prediction time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VelkEVgnvXxX",
        "colab_type": "code",
        "outputId": "7584bd67-a52a-4d5a-80a4-693e4ecce765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "start1=time.clock()\n",
        "rfc_preds1=rfc_model1.transform(test)\n",
        "end1=time.clock()\n",
        "print(\"predict:\",end1-start1)\n",
        "rfc_eval1=BinaryClassificationEvaluator(labelCol=\"labelindex\")\n",
        "print(rfc_eval1.evaluate(rfc_preds1)*100)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict: 0.01128499999999999\n",
            "77.6536111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSkdQwj7KtOk",
        "colab_type": "text"
      },
      "source": [
        "#<font color =blue> Classification Report and Confusion matrix for the best algorithm - Random Forest\n",
        "\n",
        "<font color=blue>we acheived the best accuracy with 87, 94 subset using Random forest. Therefore we are printing the confusion matrix and classification report for the best subset pair.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvBCD-je8XB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59c2u3Cxe_kV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true1=rfc_preds1.select(['labelindex']).rdd.collect()\n",
        "y_pred1=rfc_preds1.select(['prediction']).rdd.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP8f-yV6fCzv",
        "colab_type": "code",
        "outputId": "7d55bdea-36d3-4f9a-be83-c918c7ca64ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_true1,y_pred1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.64      0.69       600\n",
            "         1.0       0.68      0.77      0.72       600\n",
            "\n",
            "    accuracy                           0.70      1200\n",
            "   macro avg       0.71      0.70      0.70      1200\n",
            "weighted avg       0.71      0.70      0.70      1200\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ede3Zf1fGWi",
        "colab_type": "code",
        "outputId": "266798b0-bfcc-444e-f1ba-1dd02aa1b6a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(confusion_matrix(y_true1,y_pred1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[386 214]\n",
            " [141 459]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFKrn_pnfI4_",
        "colab_type": "code",
        "outputId": "aabd7ada-36a6-4e1f-80eb-52a3c83ae0c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "confusion_df = pd.DataFrame(confusion_matrix(y_true1,y_pred1),\n",
        "             columns=[\"Predicted Class \" + str(class_name) for class_name in ['Electronics','Furniture']],\n",
        "             index = [\"Class \" + str(class_name) for class_name in ['Electronics','Furniture']])\n",
        "print(confusion_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   Predicted Class Electronics  Predicted Class Furniture\n",
            "Class Electronics                          386                        214\n",
            "Class Furniture                            141                        459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6elQI59XvT-z",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Logistic Regression\n",
        "\n",
        "<font color=blue>same way how we did for Random forest, we did using logistic regression for all subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGOinGoDYSnA",
        "colab_type": "code",
        "outputId": "880ce2fa-ad9d-497a-e625-deec8c8116b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "for i in [22,39,40,86,87]:\n",
        "  for j in [5,20,25,84,94]:\n",
        "    train_df = data.filter(~col('fine_labels').isin([i,j]))\n",
        "    test_df  = data.filter(col('fine_labels').isin([i,j]))\n",
        "    lr = LogisticRegression(labelCol=\"labelindex\", featuresCol=\"data\")\n",
        "    start=time.clock()\n",
        "    lr_model=lr.fit(train_df)\n",
        "    end=time.clock()\n",
        "    print(\"train_time:\",end-start)\n",
        "    start1=time.clock()\n",
        "    lr_preds=lr_model.transform(test_df)\n",
        "    end1=time.clock()\n",
        "    print(\"predict_time:\",end1-start1)\n",
        "    lr_eval=BinaryClassificationEvaluator(labelCol=\"labelindex\")\n",
        "\n",
        "    print (\"Logistic Regression: {}%,{},{}\".format(lr_eval.evaluate(lr_preds)*100,i,j))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_time: 0.013232000000000355\n",
            "predict_time: 0.00631499999999896\n",
            "Logistic Regression: 62.42333333333334%,22,5\n",
            "train_time: 0.011041999999999774\n",
            "predict_time: 0.005202000000000595\n",
            "Logistic Regression: 56.41388888888888%,22,20\n",
            "train_time: 0.018952999999999776\n",
            "predict_time: 0.007177000000000433\n",
            "Logistic Regression: 58.72666666666675%,22,25\n",
            "train_time: 0.01873900000000006\n",
            "predict_time: 0.006850999999999274\n",
            "Logistic Regression: 61.83111111111115%,22,84\n",
            "train_time: 0.012542999999999083\n",
            "predict_time: 0.00616299999999903\n",
            "Logistic Regression: 71.50194444444435%,22,94\n",
            "train_time: 0.017182999999999282\n",
            "predict_time: 0.0066389999999998395\n",
            "Logistic Regression: 62.98833333333336%,39,5\n",
            "train_time: 0.015082000000001372\n",
            "predict_time: 0.003690000000000637\n",
            "Logistic Regression: 59.99111111111115%,39,20\n",
            "train_time: 0.01533500000000032\n",
            "predict_time: 0.005461000000000382\n",
            "Logistic Regression: 60.755000000000024%,39,25\n",
            "train_time: 0.01672399999999996\n",
            "predict_time: 0.005551999999999779\n",
            "Logistic Regression: 63.61361111111117%,39,84\n",
            "train_time: 0.018326999999999316\n",
            "predict_time: 0.006280999999999537\n",
            "Logistic Regression: 64.8472222222222%,39,94\n",
            "train_time: 0.013086000000001263\n",
            "predict_time: 0.008632000000000417\n",
            "Logistic Regression: 60.507777777777825%,40,5\n",
            "train_time: 0.018140999999999963\n",
            "predict_time: 0.006266000000000105\n",
            "Logistic Regression: 55.35527777777776%,40,20\n",
            "train_time: 0.018110999999999322\n",
            "predict_time: 0.008912000000000475\n",
            "Logistic Regression: 57.61111111111112%,40,25\n",
            "train_time: 0.014995000000000758\n",
            "predict_time: 0.0087160000000015\n",
            "Logistic Regression: 62.88805555555559%,40,84\n",
            "train_time: 0.016133999999999205\n",
            "predict_time: 0.004110000000000724\n",
            "Logistic Regression: 69.23444444444439%,40,94\n",
            "train_time: 0.015294000000000807\n",
            "predict_time: 0.005523999999999418\n",
            "Logistic Regression: 61.95805555555564%,86,5\n",
            "train_time: 0.013555000000000206\n",
            "predict_time: 0.007963000000000164\n",
            "Logistic Regression: 63.77527777777776%,86,20\n",
            "train_time: 0.016994000000000398\n",
            "predict_time: 0.006428000000001433\n",
            "Logistic Regression: 58.16722222222225%,86,25\n",
            "train_time: 0.017328000000000898\n",
            "predict_time: 0.0055309999999995085\n",
            "Logistic Regression: 60.872222222222284%,86,84\n",
            "train_time: 0.01203499999999913\n",
            "predict_time: 0.008606000000000336\n",
            "Logistic Regression: 68.09944444444439%,86,94\n",
            "train_time: 0.015126000000000417\n",
            "predict_time: 0.005670000000000286\n",
            "Logistic Regression: 63.40805555555551%,87,5\n",
            "train_time: 0.019263999999999726\n",
            "predict_time: 0.006534999999999513\n",
            "Logistic Regression: 58.93361111111112%,87,20\n",
            "train_time: 0.013487999999998834\n",
            "predict_time: 0.007156999999999414\n",
            "Logistic Regression: 64.42638888888888%,87,25\n",
            "train_time: 0.020229000000000497\n",
            "predict_time: 0.005527000000000726\n",
            "Logistic Regression: 62.47833333333336%,87,84\n",
            "train_time: 0.015149000000000967\n",
            "predict_time: 0.0036079999999998336\n",
            "Logistic Regression: 70.23861111111108%,87,94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qmn2Ak9evgDI",
        "colab_type": "text"
      },
      "source": [
        "##<font color =blue> Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6VewWp_uoqD",
        "colab_type": "code",
        "outputId": "0f896cbb-daf0-4527-f499-5d562ae06648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "for i in [22,39,40,86,87]:\n",
        "  for j in [5,20,25,84,94]:\n",
        "    train_df = data.filter(~col('fine_labels').isin([i,j]))\n",
        "    test_df  = data.filter(col('fine_labels').isin([i,j]))\n",
        "    nb = NaiveBayes(featuresCol=\"data\", labelCol=\"labelindex\")\n",
        "    start=time.clock()\n",
        "    nb_model=nb.fit(train_df)\n",
        "    end=time.clock()\n",
        "    print(\"train_time :\",end-start)\n",
        "    start1=time.clock()\n",
        "    nb_preds=nb_model.transform(test_df)\n",
        "    end1=time.clock()\n",
        "    print(\"prediction_time:\",end-start)\n",
        "    nb_eval=BinaryClassificationEvaluator(labelCol=\"labelindex\")\n",
        "\n",
        "    print (\"Naive Bayes: {}%,{},{}\".format(nb_eval.evaluate(nb_preds)*100,i,j))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_time : 0.007227999999999568\n",
            "prediction_time: 0.007227999999999568\n",
            "Naive Bayes: 51.524999999999984%,22,5\n",
            "train_time : 0.010128999999999166\n",
            "prediction_time: 0.010128999999999166\n",
            "Naive Bayes: 22.319722222222232%,22,20\n",
            "train_time : 0.010854000000000141\n",
            "prediction_time: 0.010854000000000141\n",
            "Naive Bayes: 56.80750000000003%,22,25\n",
            "train_time : 0.008127999999999247\n",
            "prediction_time: 0.008127999999999247\n",
            "Naive Bayes: 57.856944444444515%,22,84\n",
            "train_time : 0.005967999999999307\n",
            "prediction_time: 0.005967999999999307\n",
            "Naive Bayes: 61.14972222222224%,22,94\n",
            "train_time : 0.007241999999999749\n",
            "prediction_time: 0.007241999999999749\n",
            "Naive Bayes: 50.07277777777779%,39,5\n",
            "train_time : 0.007900000000001128\n",
            "prediction_time: 0.007900000000001128\n",
            "Naive Bayes: 22.3813888888889%,39,20\n",
            "train_time : 0.005492999999999526\n",
            "prediction_time: 0.005492999999999526\n",
            "Naive Bayes: 55.66666666666668%,39,25\n",
            "train_time : 0.008906000000001413\n",
            "prediction_time: 0.008906000000001413\n",
            "Naive Bayes: 56.73888888888894%,39,84\n",
            "train_time : 0.011039999999999495\n",
            "prediction_time: 0.011039999999999495\n",
            "Naive Bayes: 59.980277777777815%,39,94\n",
            "train_time : 0.011060999999999765\n",
            "prediction_time: 0.011060999999999765\n",
            "Naive Bayes: 47.88361111111112%,40,5\n",
            "train_time : 0.006572999999999496\n",
            "prediction_time: 0.006572999999999496\n",
            "Naive Bayes: 22.552222222222223%,40,20\n",
            "train_time : 0.007282999999999262\n",
            "prediction_time: 0.007282999999999262\n",
            "Naive Bayes: 53.10777777777776%,40,25\n",
            "train_time : 0.006878999999999635\n",
            "prediction_time: 0.006878999999999635\n",
            "Naive Bayes: 53.77388888888894%,40,84\n",
            "train_time : 0.005777000000000143\n",
            "prediction_time: 0.005777000000000143\n",
            "Naive Bayes: 56.89472222222225%,40,94\n",
            "train_time : 0.011976000000000653\n",
            "prediction_time: 0.011976000000000653\n",
            "Naive Bayes: 64.70638888888884%,86,5\n",
            "train_time : 0.008765999999999607\n",
            "prediction_time: 0.008765999999999607\n",
            "Naive Bayes: 32.93388888888889%,86,20\n",
            "train_time : 0.010464999999999947\n",
            "prediction_time: 0.010464999999999947\n",
            "Naive Bayes: 68.81277777777765%,86,25\n",
            "train_time : 0.008507999999999072\n",
            "prediction_time: 0.008507999999999072\n",
            "Naive Bayes: 69.31499999999998%,86,84\n",
            "train_time : 0.006973999999999592\n",
            "prediction_time: 0.006973999999999592\n",
            "Naive Bayes: 72.89305555555546%,86,94\n",
            "train_time : 0.0066040000000011645\n",
            "prediction_time: 0.0066040000000011645\n",
            "Naive Bayes: 38.20249999999999%,87,5\n",
            "train_time : 0.007919999999998595\n",
            "prediction_time: 0.007919999999998595\n",
            "Naive Bayes: 13.991111111111115%,87,20\n",
            "train_time : 0.008077000000000112\n",
            "prediction_time: 0.008077000000000112\n",
            "Naive Bayes: 44.47444444444445%,87,25\n",
            "train_time : 0.01143800000000006\n",
            "prediction_time: 0.01143800000000006\n",
            "Naive Bayes: 45.38833333333335%,87,84\n",
            "train_time : 0.012900999999999385\n",
            "prediction_time: 0.012900999999999385\n",
            "Naive Bayes: 48.49249999999998%,87,94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFXJ3jofMEnA",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Training and prediction time for Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acrZEw8gMP9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "train_df = data.filter(~col('fine_labels').isin([87,94]))\n",
        "test_df  = data.filter(col('fine_labels').isin([87,94]))\n",
        "nb = NaiveBayes(featuresCol=\"data\", labelCol=\"labelindex\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWDdu7vZMfkt",
        "colab_type": "code",
        "outputId": "2e859cc3-940f-46ad-cafa-3bd138b780c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "nb_model=nb.fit(train_df)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.8 ms, sys: 2.15 ms, total: 14 ms\n",
            "Wall time: 3.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aw8WyiJMmzn",
        "colab_type": "code",
        "outputId": "414fa539-1a2e-4d4c-dc98-1300f4730b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "nb_preds=nb_model.transform(test_df)\n",
        "nb_eval=BinaryClassificationEvaluator(labelCol=\"labelindex\")\n",
        "print (nb_eval.evaluate(nb_preds)*100)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48.492499999999986\n",
            "CPU times: user 11.4 ms, sys: 2.06 ms, total: 13.5 ms\n",
            "Wall time: 1.97 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_he4BFxAMRLA",
        "colab_type": "text"
      },
      "source": [
        "##<font color=blue> Gradient Boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vwNAvmlvlsq",
        "colab_type": "code",
        "outputId": "03672296-b043-4542-8f41-1c631e84fbf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import time\n",
        "from pyspark.ml.classification import (GBTClassifier)\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "for i in [22,39,40,86,87]:\n",
        "  for j in [5,20,25,84,94]:\n",
        "    train_df = data.filter(~col('fine_labels').isin([i,j]))\n",
        "    test_df  = data.filter(col('fine_labels').isin([i,j]))\n",
        "    gbt=GBTClassifier(featuresCol=\"data\", labelCol=\"labelindex\")\n",
        "    \n",
        "    start=time.clock()\n",
        "    gbt_model=gbt.fit(train_df)\n",
        "    end=time.clock()\n",
        "    print(\"train_time:\",end-start)\n",
        "    start1=time.clock()\n",
        "    gbt_preds=gbt_model.transform(test_df)\n",
        "    end1=time.clock()\n",
        "    print(\"predict_time:\",end1-start1)\n",
        "    gbt_eval=BinaryClassificationEvaluator(labelCol=\"labelindex\")\n",
        "\n",
        "    print (\"Gradient Boosting: {}%,{},{}\".format(gbt_eval.evaluate(gbt_preds)*100,i,j))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_time: 0.14236399999999882\n",
            "predict_time: 0.005954000000000903\n",
            "Gradient Boosting: 64.08444444444451%,22,5\n",
            "train_time: 0.07782100000000014\n",
            "predict_time: 0.007023999999999475\n",
            "Gradient Boosting: 57.298055555555536%,22,20\n",
            "train_time: 0.07887099999999947\n",
            "predict_time: 0.005981000000000236\n",
            "Gradient Boosting: 60.43638888888893%,22,25\n",
            "train_time: 0.07381899999999852\n",
            "predict_time: 0.005981000000000236\n",
            "Gradient Boosting: 65.8425%,22,84\n",
            "train_time: 0.08484499999999962\n",
            "predict_time: 0.009094000000001046\n",
            "Gradient Boosting: 71.08305555555552%,22,94\n",
            "train_time: 0.07921600000000062\n",
            "predict_time: 0.009460999999999942\n",
            "Gradient Boosting: 65.42833333333337%,39,5\n",
            "train_time: 0.08305000000000007\n",
            "predict_time: 0.007885999999999171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-GBqR3McO8",
        "colab_type": "text"
      },
      "source": [
        "#<font color= blue> Training and prediction time for Gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ-WR0yMdGH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import (GBTClassifier)\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "train_df = data.filter(~col('fine_labels').isin([87,94]))\n",
        "test_df  = data.filter(col('fine_labels').isin([87,94]))\n",
        "gbt=GBTClassifier(featuresCol=\"data\", labelCol=\"labelindex\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6DHuN6tYqTC",
        "colab_type": "code",
        "outputId": "574bb8ce-00e6-40d1-e4cc-a390d2614d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "start=time.time()\n",
        "gbt_model=gbt.fit(train_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 152 ms, sys: 40.1 ms, total: 192 ms\n",
            "Wall time: 12min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWgCvGpSdmhH",
        "colab_type": "code",
        "outputId": "5a59f75c-47c2-4d44-e3dc-7a834e2966de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "gbt_preds=gbt_model.transform(test_df)\n",
        "gbt_eval=BinaryClassificationEvaluator(labelCol=\"labelindex\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11.8 ms, sys: 3.55 ms, total: 15.4 ms\n",
            "Wall time: 124 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzxlVmQdaDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
